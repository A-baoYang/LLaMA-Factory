version: '3.8'

services:
  llama-factory:
    image: llama-factory-llama-factory
    # build:
    #   dockerfile: Dockerfile
    #   context: .
    container_name: llama_factory
    volumes:
      - ./hf_cache:/root/.cache/huggingface/
      - ./data:/app/data
      - ./output:/app/output
      - ./saves:/app/saves
      - ./examples:/app/examples
      - ./evaluation:/app/evaluation
    environment:
      - CUDA_VISIBLE_DEVICES=0,1
      - HF_HUB_TOKEN=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
    ports:
      - "7860:7860"
      - "8000:8000"
    ipc: host
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            count: "all"
            capabilities: [gpu]
    restart: unless-stopped
    # command: ["llamafactory-cli", "api", "examples/inference/commandr01.yaml"]
